{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/OneDrive/data-science/github/isuelogit\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# External modules\n",
    "import ast\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n",
    "\n",
    "# Path management\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get main project directory \n",
    "main_dir = str(Path(os.path.abspath('')).parents[0])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:',main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Internal modules\n",
    "from src import isuelogit as isl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read network data from Fresno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/pablo/OneDrive/university/cmu/1-courses/2021-1/10605-ml-with-ld/software/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/04 12:43:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "network_name = 'Fresno'\n",
    "\n",
    "# Reader of geospatial and spatio-temporal data\n",
    "data_reader = isl.etl.DataReader(network_key=network_name,setup_spark=True)\n",
    "\n",
    "# Read files\n",
    "links_df, nodes_df = isl.reader.read_fresno_network(folderpath=isl.dirs['Fresno_network'])\n",
    "\n",
    "nodes_df.to_csv(isl.dirs['output_folder'] + '/network-data/nodes/'  + 'fresno-nodes-data.csv',\n",
    "                sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "# Add link key in dataframe\n",
    "links_df['link_key'] = [(int(i), int(j), '0') for i, j in zip(links_df['init_node_key'], links_df['term_node_key'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Fresno network\n",
      "\n",
      "Nodes: 1789, Links: 2413\n"
     ]
    }
   ],
   "source": [
    "network_generator = isl.factory.NetworkGenerator()\n",
    "\n",
    "A = network_generator.generate_adjacency_matrix(links_keys=list(links_df.link_key.values))\n",
    "\n",
    "fresno_network = \\\n",
    "    network_generator.build_fresno_network(A=A, links_df=links_df, nodes_df=nodes_df, network_name= network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exogenous link features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data on link features from network file\n",
    "link_features_df = links_df[['link_key', 'id', 'link_type', 'rhoj', 'lane', 'ff_speed', 'length']]\n",
    "\n",
    "# Attributes\n",
    "link_features_df['link_type'] = link_features_df['link_type'].apply(lambda x: x.strip())\n",
    "link_features_df['rhoj'] = pd.to_numeric(link_features_df['rhoj'], errors='coerce', downcast='float')\n",
    "link_features_df['lane'] = pd.to_numeric(link_features_df['lane'], errors='coerce', downcast='integer')\n",
    "link_features_df['length'] = pd.to_numeric(link_features_df['length'], errors='coerce', downcast='float')\n",
    "\n",
    "# Load features data\n",
    "fresno_network.load_features_data(linkdata=link_features_df, link_key = 'link_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Performance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'tt_units': 'minutes'}\n",
    "\n",
    "# Create two new features\n",
    "if options['tt_units'] == 'minutes':\n",
    "    # Weighting by 60 will leave travel time with minutes units, because speeds are originally in per hour units\n",
    "    tt_factor = 60\n",
    "\n",
    "if options['tt_units'] == 'seconds':\n",
    "    tt_factor = 60 * 60\n",
    "\n",
    "links_df['ff_speed'] = pd.to_numeric(links_df['ff_speed'], errors='coerce', downcast='float')\n",
    "links_df['ff_traveltime'] = tt_factor * links_df['length'] / links_df['ff_speed']\n",
    "\n",
    "bpr_parameters_df = pd.DataFrame({'link_key': links_df['link_key'],\n",
    "                                  'alpha': 0.15,\n",
    "                                  'beta': 4,\n",
    "                                  'tf': links_df['ff_traveltime'],\n",
    "                                  'k': pd.to_numeric(links_df['capacity'], errors='coerce', downcast='float')\n",
    "                                  })\n",
    "\n",
    "fresno_network.set_bpr_functions(bprdata=bpr_parameters_df, link_key = 'link_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and write OD Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.8[s]\n",
      "66266.3 trips were loaded among 6970 o-d pairs\n",
      "Matrix Q (1789, 1789) written in 0.0[s] with sparse format\n"
     ]
    }
   ],
   "source": [
    "# - Periods (6 periods of 15 minutes each)\n",
    "data_reader.options['od_periods'] = [1, 2, 3, 4]\n",
    "\n",
    "# Read OD from raw data\n",
    "Q = isl.reader.read_fresno_dynamic_od(network=fresno_network,\n",
    "                                  filepath=isl.dirs['Fresno_network'] + '/SR41.dmd',\n",
    "                                  periods=data_reader.options['od_periods'])\n",
    "\n",
    "network_generator.write_OD_matrix(network = fresno_network, sparse = True, overwrite_input=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatiotemporal features and traffic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected date is 2019-10-01, Tuesday at 16:00\n",
      "            perc  count\n",
      "link_types             \n",
      "LWRLK      71.2%   1717\n",
      "PQULK      14.4%    348\n",
      "DMDLK       7.2%    174\n",
      "DMOLK       7.2%    174\n",
      "\n",
      "Matching geospatial datasets to links with type \"LWRLK\" \n",
      "\n",
      "Reading network shapefile generated from x,y coordinates and qgis\n",
      "\n",
      "Reading inrix shapefile of Fresno\n",
      "Matching INRIX segments (N=13417) with network links\n",
      "1468 network links were matched (85.5% of links) with a 85.3% confidence\n",
      "\n",
      "Reading and processing INRIX data with pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/04 12:45:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 0:=====================================================>   (48 + 3) / 51]\r"
     ]
    }
   ],
   "source": [
    "# 2019-10-01: First Tuesday of October, 2019\n",
    "# 2020-10-06: First Tuesday of October, 2020\n",
    "dates = ['2019-10-01','2020-10-06']\n",
    "\n",
    "lwrlk_only = True\n",
    "\n",
    "for date in dates:\n",
    "\n",
    "    data_reader.select_period(date=date, hour=16)\n",
    "\n",
    "    # =============================================================================\n",
    "    # SPATIO-TEMPORAL LINK FEATURES\n",
    "    # =============================================================================\n",
    "\n",
    "    spatiotemporal_features_df, spatiotemporal_features_list = data_reader.read_spatiotemporal_data_fresno(\n",
    "        lwrlk_only=True,\n",
    "        read_inrix_daily_data = False, #When false, it is read all data from October 2019 and 2020. When true, only data from selected date is read\n",
    "        network=fresno_network,\n",
    "        selected_period_incidents={'year': [data_reader.options['selected_year']],'month': [9, 10]},\n",
    "        selected_period_inrix = {'year': [data_reader.options['selected_year']],\n",
    "                                 'month': [data_reader.options['selected_month']],\n",
    "                                 # 'day_month': [self.options['selected_day_month']],\n",
    "                                 'day_week': [1, 2, 3, 4, 5],\n",
    "                                 'hour': [data_reader.options['selected_hour'] - 1,\n",
    "                                          data_reader.options['selected_hour'], data_reader.options['selected_hour'] + 1]},\n",
    "        data_processing={'inrix_segments': True, 'inrix_data': True, 'census': True, 'incidents': True,\n",
    "                         'bus_stops': True, 'streets_intersections': True},\n",
    "        # data_processing={'inrix_segments': False, 'inrix_data': False, 'census': False, 'incidents': False,\n",
    "        #                  'bus_stops': False, 'streets_intersections': False},\n",
    "        inrix_matching={'census': False, 'incidents': False, 'bus_stops': False, 'streets_intersections': False},\n",
    "        # inrix_matching={'census': False, 'incidents': False, 'bus_stops': False, 'streets_intersections': False},\n",
    "        # buffer_size={'inrix': 200, 'bus_stops': 50, 'incidents': 50, 'streets_intersections': 50},\n",
    "        buffer_size={'inrix': 100, 'bus_stops': 50, 'incidents': 50, 'streets_intersections': 50},\n",
    "        tt_units='minutes'\n",
    "    )\n",
    "\n",
    "    filepath = isl.dirs['output_folder'] + '/network-data/links/' + str(data_reader.options['selected_date']) \\\n",
    "           + '-fresno-spatiotemporal-link-data.csv'\n",
    "\n",
    "    spatiotemporal_features_df.to_csv(filepath, sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    # Test Reader\n",
    "    spatiotemporal_features_df = pd.read_csv(filepath)\n",
    "\n",
    "    fresno_network.load_features_data(spatiotemporal_features_df)\n",
    "\n",
    "    # =============================================================================\n",
    "    # d) FREE FLOW TRAVEL TIME FROM INRIX\n",
    "    # =============================================================================\n",
    "\n",
    "    for link in fresno_network.links:\n",
    "        if link.link_type == 'LWRLK' and link.Z_dict['speed_ref_avg']!=0:\n",
    "            # Multiplied by 60 so speeds are in minutes\n",
    "            link.Z_dict['tf_inrix'] = tt_factor * link.Z_dict['length'] / link.Z_dict['speed_max']\n",
    "#             link.bpr.tf = tt_factor * link.Z_dict['length'] / link.Z_dict['speed_ref_avg']\n",
    "        else:\n",
    "            link.Z_dict['tf_inrix'] = link.bpr.tf \n",
    "            #= links_df[links_df['link_key'].astype(str) == str(link.key)]['ff_traveltime']\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3c) DATA CURATION\n",
    "    # =============================================================================\n",
    "\n",
    "    # a) Imputation to correct for outliers and observations with zero values because no GIS matching\n",
    "\n",
    "    features_list = ['median_inc', 'intersections', 'incidents', 'bus_stops', 'median_age',\n",
    "                     'tt_avg', 'tt_sd','tt_var', 'tt_cv',\n",
    "                     'speed_ref_avg', 'speed_avg','speed_sd','speed_cv']\n",
    "\n",
    "    for feature in features_list:\n",
    "        fresno_network.link_data.feature_imputation(feature =feature, pcts = (0, 100),lwrlk_only=lwrlk_only)\n",
    "\n",
    "    # b) Feature values in \"connectors\" links\n",
    "    for key in features_list:\n",
    "        for link in fresno_network.get_non_regular_links():\n",
    "            link.Z_dict[key] = 0\n",
    "    print('Features values of link with types different than \"LWRLK\" were set to 0')\n",
    "\n",
    "    # a) Capacity adjustment\n",
    "\n",
    "    # counts = isl.etl.adjust_counts_by_link_capacity(network = fresno_network, counts = counts)\n",
    "\n",
    "    # b) Outliers\n",
    "\n",
    "    # isl.etl.remove_outliers_fresno(fresno_network)\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2) TRAFFIC COUNTS\n",
    "    # =============================================================================\n",
    "\n",
    "    # ii) Read data from PEMS count and perform matching GIS operations to combine station shapefiles\n",
    "\n",
    "    date_pathname = data_reader.options['selected_date'].replace('-', '_')\n",
    "\n",
    "    path_pems_counts = isl.dirs['input_folder'] + 'public/pems/counts/data/' + \\\n",
    "                       'd06_text_station_5min_' + date_pathname + '.txt.gz'\n",
    "\n",
    "    # Load pems station ids in links\n",
    "    isl.etl.load_pems_stations_ids(network=fresno_network)\n",
    "\n",
    "    # Read and match count data from a given period\n",
    "    count_interval_df \\\n",
    "        = data_reader.read_pems_counts_by_period(\n",
    "        filepath=path_pems_counts,\n",
    "        selected_period={'hour': data_reader.options['selected_hour'],\n",
    "                         'duration': int(len(data_reader.options['od_periods']) * 15)})\n",
    "\n",
    "    # Generate a masked vector that fill out count values with no observations with nan\n",
    "    counts = isl.etl.generate_fresno_pems_counts(links=fresno_network.links\n",
    "                                                 , data=count_interval_df\n",
    "                                                 # , flow_attribute='flow_total'\n",
    "                                                 # , flow_attribute = 'flow_total_lane_1')\n",
    "                                                 , flow_attribute='flow_total_lane'\n",
    "                                                 , flow_factor=1  # 0.1\n",
    "                                                 )\n",
    "    # Write counts in csv\n",
    "\n",
    "    filepath = isl.dirs['output_folder'] + 'network-data/links/' + str(data_reader.options['selected_date']) \\\n",
    "               + '-fresno-link-counts.csv'\n",
    "\n",
    "    counts_df = pd.DataFrame({'link_key': counts.keys(),\n",
    "                              'counts': counts.values(),\n",
    "                              'pems_ids': [link.pems_stations_ids for link in fresno_network.links]})\n",
    "    counts_df.to_csv(filepath, sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    # Read counts from csv\n",
    "    counts_df = pd.read_csv(filepath, converters={\"link_key\": ast.literal_eval})\n",
    "\n",
    "    counts = dict(zip(counts_df['link_key'].values, counts_df['counts'].values))\n",
    "\n",
    "    # Load counts\n",
    "    fresno_network.load_traffic_counts(counts=counts)\n",
    "\n",
    "   # =============================================================================\n",
    "    # c) WRITE FILE WITH THE ADJUSTED GIS POSITIONS OF NODES\n",
    "    # =============================================================================\n",
    "\n",
    "    # Update coordinates of raw file\n",
    "    nodes_df['lon'] = [node.position.get_xy()[0] for node in fresno_network.nodes]\n",
    "    nodes_df['lat'] = [node.position.get_xy()[1] for node in fresno_network.nodes]\n",
    "\n",
    "    nodes_df.to_csv(isl.dirs['output_folder'] + '/network-data/nodes/' + 'fresno-nodes-data.csv',\n",
    "                    sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    isl.geographer.write_nodes_gdf(nodes_df,\n",
    "                                   folderpath=isl.config.dirs['output_folder'] + 'gis/Fresno/network/nodes',\n",
    "                                   filename='Fresno_nodes.shp')\n",
    "\n",
    "    # =============================================================================\n",
    "    # d) WRITE FILE WITH LINK FEATURES AND COUNTS\n",
    "    # =============================================================================\n",
    "\n",
    "    summary_table_links_df = isl.descriptive_statistics.summary_table_links(links=fresno_network.links)\n",
    "\n",
    "    summary_table_links_df.to_csv(isl.dirs['output_folder'] + 'network-data/links/'\n",
    "                                  + str(data_reader.options['selected_date']) + '-fresno-link-data.csv',\n",
    "                                  sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    folderpath = isl.dirs['output_folder'] + 'gis/Fresno/features/' + data_reader.options['selected_date']\n",
    "\n",
    "    isl.geographer.write_links_features_map_shp(network=fresno_network,\n",
    "                                                folderpath=folderpath,\n",
    "                                                filename='link_features_' + data_reader.options[\n",
    "                                                    'selected_date'] + '.shp'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and write Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create path generator\n",
    "paths_generator = isl.factory.PathsGenerator()\n",
    "\n",
    "# Generate and Load paths in network\n",
    "paths_generator.load_k_shortest_paths(network = fresno_network, k=2)\n",
    "# \n",
    "# Write paths and incidence matrices\n",
    "paths_generator.write_paths(network=fresno_network, overwrite_input=False)\n",
    "\n",
    "network_generator.write_incidence_matrices(network = fresno_network,\n",
    "                                          matrices = {'sparse_C':True, 'sparse_D':True, 'sparse_M':True},\n",
    "                                          overwrite_input = False)\n",
    "\n",
    "paths_generator.read_paths(network=fresno_network, update_incidence_matrices=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "venv-isuelogit",
   "language": "python",
   "display_name": "venv-isuelogit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}