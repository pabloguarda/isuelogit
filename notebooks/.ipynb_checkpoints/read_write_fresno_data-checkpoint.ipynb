{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main dir: /Users/pablo/OneDrive/data-science/github/transportAI\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Path management\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get main project directory \n",
    "main_dir = str(Path(os.path.abspath('')).parents[0])\n",
    "os.chdir(main_dir)\n",
    "print('main dir:',main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_dir: /Users/pablo/OneDrive/data-science/github/transportAI\n"
     ]
    }
   ],
   "source": [
    "# Internal modules\n",
    "from src import transportAI as tai\n",
    "\n",
    "# External modules\n",
    "import ast\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2) NETWORK FACTORY\n",
    "# ============================================================================\n",
    "network_name = 'Fresno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/pablo/OneDrive/university/cmu/1-courses/2021-1/10605-ml-with-ld/software/spark-3.0.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/21 13:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# a) READ FRESNO LINK DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Reader of geospatial and spatio-temporal data\n",
    "data_reader = tai.etl.DataReader(network_key=network_name,setup_spark=True)\n",
    "\n",
    "# Read files\n",
    "links_df, nodes_df = tai.reader.read_fresno_network(folderpath=tai.dirs['Fresno_network'])\n",
    "\n",
    "nodes_df.to_csv(tai.dirs['output_folder'] + '/network-data/nodes/'  + 'fresno-nodes-data.csv',\n",
    "                sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "# Add link key in dataframe\n",
    "links_df['link_key'] = [(int(i), int(j), '0') for i, j in zip(links_df['init_node_key'], links_df['term_node_key'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Fresno network\n",
      "\n",
      "Nodes: 1789, Links: 2413\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# a) BUILD NETWORK\n",
    "# =============================================================================\n",
    "\n",
    "# Create Network Generator\n",
    "network_generator = tai.factory.NetworkGenerator()\n",
    "\n",
    "A = network_generator.generate_adjacency_matrix(links_keys=list(links_df.link_key.values))\n",
    "\n",
    "fresno_network = \\\n",
    "    network_generator.build_fresno_network(A=A, links_df=links_df, nodes_df=nodes_df, network_name= network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Q (1789, 1789) read in 0.5[s]\n",
      "Trips from 6970 o-d pairs were loaded\n",
      "Matrix Q (1789, 1789) written in 0.0[s] with sparse format\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# f) OD\n",
    "# =============================================================================\n",
    "\n",
    "# - Periods (6 periods of 15 minutes each)\n",
    "data_reader.options['od_periods'] = [1, 2, 3, 4]\n",
    "\n",
    "# Read OD from raw data\n",
    "Q = tai.reader.read_fresno_dynamic_od(network=fresno_network,\n",
    "                                  filepath=tai.dirs['Fresno_network'] + '/SR41.dmd',\n",
    "                                  periods=data_reader.options['od_periods'])\n",
    "\n",
    "network_generator.write_OD_matrix(network = fresno_network, sparse = True, overwrite_input=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating paths\n",
      "Generating 4 paths per od\n",
      "27774 paths were generated among 6970 od pairs\n",
      "Computation time: 189.1 [s]\n",
      "27774 paths were loaded\n",
      "Updating incident matrices\n",
      "Progress(D): |████████████████████| 100.0% \n",
      "Matrix D (2413, 27774) generated in 29.4[s]\n",
      "Progress(M): |████████████████████| 100.0% \n",
      "Matrix M (6970, 27774) generated in 1.2[s]\n",
      "Matrix C (27774, 27774) generated in 6.2[s]\n",
      "Progress (paths): |████████████████████| 100.0% \n",
      "27774 paths were written in 5.2[s]\n",
      "Matrix C (27774, 27774) written in 7.7[s] with sparse format\n",
      "Matrix D (2413, 27774) written in 1.2[s] with sparse format\n",
      "Matrix M (6970, 27774) written in 2.0[s] with sparse format\n",
      "26380 paths were read in 7.3[s]| 100.0% \n",
      "26380 paths were loaded\n",
      "Updating incident matrices\n",
      "Progress(D): |████████████████████| 100.0% \n",
      "Matrix D (2413, 26380) generated in 27.8[s]\n",
      "Progress(M): |████████████████████| 100.0% \n",
      "Matrix M (6970, 26380) generated in 1.2[s]\n",
      "Matrix C (26380, 26380) generated in 6.0[s]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# g) PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Create path generator\n",
    "paths_generator = tai.factory.PathsGenerator()\n",
    "\n",
    "# Generate and Load paths in network\n",
    "paths_generator.load_k_shortest_paths(network = fresno_network, k=4)\n",
    "# \n",
    "# Write paths and incident matrices\n",
    "paths_generator.write_paths(network=fresno_network, overwrite_input=False)\n",
    "\n",
    "network_generator.write_incident_matrices(network = fresno_network,\n",
    "                                          matrices = {'sparse_C':True, 'sparse_D':True, 'sparse_M':True},\n",
    "                                          overwrite_input = False)\n",
    "\n",
    "paths_generator.read_paths(network=fresno_network, update_incidence_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# c) LINK FEATURES FROM NETWORK FILE\n",
    "# =============================================================================\n",
    "\n",
    "# Extract data on link features from network file\n",
    "link_features_df = links_df[['link_key', 'id', 'link_type', 'rhoj', 'lane', 'ff_speed', 'length']]\n",
    "\n",
    "# Attributes\n",
    "link_features_df['link_type'] = link_features_df['link_type'].apply(lambda x: x.strip())\n",
    "link_features_df['rhoj'] = pd.to_numeric(link_features_df['rhoj'], errors='coerce', downcast='float')\n",
    "link_features_df['lane'] = pd.to_numeric(link_features_df['lane'], errors='coerce', downcast='integer')\n",
    "link_features_df['length'] = pd.to_numeric(link_features_df['length'], errors='coerce', downcast='float')\n",
    "\n",
    "# Load features data\n",
    "fresno_network.load_features_data(linkdata=link_features_df, link_key = 'link_key')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# d) LINK PERFORMANCE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "options = {'tt_units': 'minutes'}\n",
    "\n",
    "# Create two new features\n",
    "if options['tt_units'] == 'minutes':\n",
    "    # Weighting by 60 will leave travel time with minutes units, because speeds are originally in per hour units\n",
    "    tt_factor = 60\n",
    "\n",
    "if options['tt_units'] == 'seconds':\n",
    "    tt_factor = 60 * 60\n",
    "\n",
    "links_df['ff_speed'] = pd.to_numeric(links_df['ff_speed'], errors='coerce', downcast='float')\n",
    "links_df['ff_traveltime'] = tt_factor * links_df['length'] / links_df['ff_speed']\n",
    "\n",
    "bpr_parameters_df = pd.DataFrame({'link_key': links_df['link_key'],\n",
    "                                  'alpha': 0.15,\n",
    "                                  'beta': 4,\n",
    "                                  'tf': links_df['ff_traveltime'],\n",
    "                                  'k': pd.to_numeric(links_df['capacity'], errors='coerce', downcast='float')\n",
    "                                  })\n",
    "\n",
    "fresno_network.set_bpr_functions(bprdata=bpr_parameters_df, link_key = 'link_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected date is 2019-10-01, Tuesday at 16:00\n",
      "\n",
      "Selected date is 2019-10-01, Tuesday at 16:00\n",
      "            perc  count\n",
      "link_types             \n",
      "LWRLK      71.2%   1717\n",
      "PQULK      14.4%    348\n",
      "DMDLK       7.2%    174\n",
      "DMOLK       7.2%    174\n",
      "\n",
      "Matching geospatial datasets to all links types\" \n",
      "\n",
      "Reading network shapefile generated from x,y coordinates and qgis\n",
      "\n",
      "Reading inrix shapefile of Fresno\n",
      "Matching INRIX segments (N=13417) with network links\n",
      "1801 network links were matched (74.6% of links) with a 78.6% confidence\n",
      "\n",
      "Reading and processing INRIX data with pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/21 13:46:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging INRIX data of speeds and travel times with network links\n",
      "\n",
      "Reading census data at the block level\n",
      "2313 network links were matched (95.9% of links)\n",
      "100 network links were imputed (4.1% of links)\n",
      "\n",
      "Reading traffic incidents data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching incidents (N=1644) with network links\n",
      "1861 incidents were matched to 471 links  (19.5% of links)\n",
      "\n",
      "Matching bus stops (N=1503) with network links\n",
      "1318 bus stops were matched to 743 links (30.8% of links)\n",
      "\n",
      "Reading shapefiles with street intersections in Fresno\n",
      "\n",
      "Matching street intersections (N=17302) with network links\n",
      "6227 street intersecions were matched to 1727 links (71.6% of links)\n",
      "Data for feature median_inc was imputed with value 37.2922 among 151 links\n",
      "Data for feature intersections was imputed with value 3.3928 among 234 links\n",
      "Data for feature incidents was imputed with value 3.0547 among 1321 links\n",
      "Data for feature bus_stops was imputed with value 1.5672 among 1107 links\n",
      "Data for feature median_age was imputed with value 31.5406 among 116 links\n",
      "Data for feature tt_avg was imputed with value 0.5172 among 309 links\n",
      "Data for feature tt_sd was imputed with value 0.1251 among 306 links\n",
      "Data for feature tt_var was imputed with value 0.0258 among 447 links\n",
      "Data for feature tt_cv was imputed with value 0.2398 among 305 links\n",
      "Data for feature speed_ref_avg was imputed with value 31.7833 among 337 links\n",
      "Data for feature speed_avg was imputed with value 31.2246 among 305 links\n",
      "Data for feature speed_hist_avg was imputed with value 29.4865 among 307 links\n",
      "Data for feature speed_sd was imputed with value 5.4204 among 305 links\n",
      "Data for feature speed_hist_sd was imputed with value 4.0632 among 305 links\n",
      "Data for feature speed_cv was imputed with value 0.2033 among 306 links\n",
      "Features values of link with types different than \"LWRLK\" were set to 0\n",
      "\n",
      "Reading network shapefile generated from x,y coordinates and qgis\n",
      "Reading pems counts starting at 16:00 and during 60 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching PEMS traffic count measurements in network links\n",
      "122 links were perfectly matched\n",
      "19 links counts were imputed using the average traffic counts among lanes\n",
      "\n",
      "Selected date is 2020-10-06, Tuesday at 16:00\n",
      "\n",
      "Selected date is 2020-10-06, Tuesday at 16:00\n",
      "            perc  count\n",
      "link_types             \n",
      "LWRLK      71.2%   1717\n",
      "PQULK      14.4%    348\n",
      "DMDLK       7.2%    174\n",
      "DMOLK       7.2%    174\n",
      "\n",
      "Matching geospatial datasets to all links types\" \n",
      "\n",
      "Reading network shapefile generated from x,y coordinates and qgis\n",
      "\n",
      "Reading inrix shapefile of Fresno\n",
      "Matching INRIX segments (N=13417) with network links\n",
      "1758 network links were matched (72.9% of links) with a 77.3% confidence\n",
      "\n",
      "Reading and processing INRIX data with pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging INRIX data of speeds and travel times with network links\n",
      "\n",
      "Reading census data at the block level\n",
      "2313 network links were matched (95.9% of links)\n",
      "100 network links were imputed (4.1% of links)\n",
      "\n",
      "Reading traffic incidents data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching incidents (N=1098) with network links\n",
      "525 incidents were matched to 320 links  (13.3% of links)\n",
      "\n",
      "Matching bus stops (N=1503) with network links\n",
      "1357 bus stops were matched to 760 links (31.5% of links)\n",
      "\n",
      "Reading shapefiles with street intersections in Fresno\n",
      "\n",
      "Matching street intersections (N=17302) with network links\n",
      "12590 street intersecions were matched to 1801 links (74.6% of links)\n",
      "Data for feature median_inc was imputed with value 37.2922 among 151 links\n",
      "Data for feature intersections was imputed with value 6.6204 among 174 links\n",
      "Data for feature incidents was imputed with value 1.4964 among 1472 links\n",
      "Data for feature bus_stops was imputed with value 1.7397 among 1095 links\n",
      "Data for feature median_age was imputed with value 31.5406 among 116 links\n",
      "Data for feature tt_avg was imputed with value 0.4586 among 126 links\n",
      "Data for feature tt_sd was imputed with value 0.0694 among 125 links\n",
      "Data for feature tt_var was imputed with value 0.0078 among 125 links\n",
      "Data for feature tt_cv was imputed with value 0.1497 among 125 links\n",
      "Data for feature speed_ref_avg was imputed with value 32.2163 among 157 links\n",
      "Data for feature speed_avg was imputed with value 32.5957 among 125 links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo/OneDrive/data-science/github/transportAI/src/transportAI/etl.py:1866: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mean = diff_sum / diff_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for feature speed_hist_avg was imputed with value nan among 0 links\n",
      "Data for feature speed_sd was imputed with value 4.0361 among 126 links\n",
      "Data for feature speed_hist_sd was imputed with value nan among 0 links\n",
      "Data for feature speed_cv was imputed with value 0.1428 among 125 links\n",
      "Features values of link with types different than \"LWRLK\" were set to 0\n",
      "\n",
      "Reading network shapefile generated from x,y coordinates and qgis\n",
      "Reading pems counts starting at 16:00 and during 60 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching PEMS traffic count measurements in network links\n",
      "122 links were perfectly matched\n",
      "19 links counts were imputed using the average traffic counts among lanes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# d) SPATIO-TEMPORAL LINK FEATURES AND TRAFFIC COUNTS\n",
    "# =============================================================================\n",
    "\n",
    "dates = ['2019-10-01','2020-10-06']\n",
    "\n",
    "options['update_ff_tt_inrix'] = True\n",
    "\n",
    "for date in dates:\n",
    "\n",
    "    # First Tuesday of October, 2019 (2019-10-01)\n",
    "    data_reader.select_period(date=date, hour=16)\n",
    "\n",
    "    # First Tuesday of October, 2020 (2020-10-06)\n",
    "    data_reader.select_period(date=date, hour=16)\n",
    "\n",
    "    # =============================================================================\n",
    "    # SPATIO-TEMPORAL LINK FEATURES\n",
    "    # =============================================================================\n",
    "\n",
    "    filepath = tai.dirs['output_folder'] + '/network-data/links/' + str(data_reader.options['selected_date']) \\\n",
    "               + '-fresno-spatiotemporal-link-data.csv'\n",
    "\n",
    "    spatiotemporal_features_df, spatiotemporal_features_list = data_reader.read_spatiotemporal_data_fresno(\n",
    "            lwrlk_only=False,\n",
    "            network=fresno_network,\n",
    "            selected_period_incidents={'year': [data_reader.options['selected_year']],\n",
    "                                       'month': [7, 8, 9, 10]},\n",
    "            data_processing={'inrix_segments': True, 'inrix_data': True, 'census': True, 'incidents': True,\n",
    "                             'bus_stops': True, 'streets_intersections': True},\n",
    "            # data_processing={'inrix_segments': False, 'inrix_data': False, 'census': False, 'incidents': False,\n",
    "            #                  'bus_stops': False, 'streets_intersections': False},\n",
    "            inrix_matching={'census': False, 'incidents': True, 'bus_stops': True, 'streets_intersections': True},\n",
    "            buffer_size={'inrix': 200, 'bus_stops': 50, 'incidents': 50, 'streets_intersections': 50},\n",
    "            tt_units='minutes'\n",
    "        )\n",
    "\n",
    "    spatiotemporal_features_df.to_csv(filepath, sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    # Test Reader\n",
    "    spatiotemporal_features_df = pd.read_csv(filepath)\n",
    "\n",
    "    fresno_network.load_features_data(spatiotemporal_features_df)\n",
    "\n",
    "    # =============================================================================\n",
    "    # d) FREE FLOW TRAVEL TIME OF LINK PERFORMANCE FUNCTIONS\n",
    "    # =============================================================================\n",
    "\n",
    "    # Create two new features\n",
    "    if options['tt_units'] == 'minutes':\n",
    "        # Weighting by 60 will leave travel time with minutes units, because speeds are originally in per hour units\n",
    "        tt_factor = 60\n",
    "\n",
    "    if options['tt_units'] == 'seconds':\n",
    "        tt_factor = 60 * 60\n",
    "\n",
    "    if options['update_ff_tt_inrix']:\n",
    "        for link in fresno_network.links:\n",
    "            if link.link_type == 'LWRLK' and link.Z_dict['speed_ref_avg']!=0:\n",
    "                # Multiplied by 60 so speeds are in minutes\n",
    "                link.bpr.tf = tt_factor * link.Z_dict['length'] / link.Z_dict['speed_max']\n",
    "                # link.bpr.tf = tt_factor * link.Z_dict['length'] / link.Z_dict['speed_ref_avg']\n",
    "                # else:\n",
    "                #     link.bpr.tf = links_df[links_df['link_key'].astype(str) == str(link.key)]['ff_traveltime']\n",
    "\n",
    "        fresno_network.set_bpr_functions(bprdata=bpr_parameters_df, link_key = 'link_key')\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3c) DATA CURATION\n",
    "    # =============================================================================\n",
    "\n",
    "    # a) Imputation to correct for outliers and observations with zero values because no GIS matching\n",
    "\n",
    "    features_list = ['median_inc', 'intersections', 'incidents', 'bus_stops', 'median_age',\n",
    "                     'tt_avg', 'tt_sd','tt_var', 'tt_cv',\n",
    "                     'speed_ref_avg', 'speed_avg','speed_sd','speed_cv']\n",
    "\n",
    "    for feature in features_list:\n",
    "        fresno_network.link_data.feature_imputation(feature =feature, pcts = (2, 98))\n",
    "\n",
    "    # b) Feature values in \"connectors\" links\n",
    "    for key in features_list:\n",
    "        for link in fresno_network.get_non_regular_links():\n",
    "            link.Z_dict[key] = 0\n",
    "    print('Features values of link with types different than \"LWRLK\" were set to 0')\n",
    "\n",
    "    # a) Capacity adjustment\n",
    "\n",
    "    # counts = tai.etl.adjust_counts_by_link_capacity(network = fresno_network, counts = counts)\n",
    "\n",
    "    # b) Outliers\n",
    "\n",
    "    # tai.etl.remove_outliers_fresno(fresno_network)\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2) TRAFFIC COUNTS\n",
    "    # =============================================================================\n",
    "\n",
    "    # ii) Read data from PEMS count and perform matching GIS operations to combine station shapefiles\n",
    "\n",
    "    date_pathname = data_reader.options['selected_date'].replace('-', '_')\n",
    "\n",
    "    path_pems_counts = tai.dirs['input_folder'] + 'public/pems/counts/data/' + \\\n",
    "                       'd06_text_station_5min_' + date_pathname + '.txt.gz'\n",
    "\n",
    "    # Load pems station ids in links\n",
    "    tai.etl.load_pems_stations_ids(network=fresno_network)\n",
    "\n",
    "    # Read and match count data from a given period\n",
    "\n",
    "    # Duration is set at 2 because the simulation time for the OD matrix was set at that value\n",
    "    count_interval_df \\\n",
    "        = data_reader.read_pems_counts_by_period(\n",
    "        filepath=path_pems_counts,\n",
    "        selected_period={'hour': data_reader.options['selected_hour'],\n",
    "                         'duration': int(len(data_reader.options['od_periods']) * 15)})\n",
    "\n",
    "    # Generate a masked vector that fill out count values with no observations with nan\n",
    "    counts = tai.etl.generate_fresno_pems_counts(links=fresno_network.links\n",
    "                                                 , data=count_interval_df\n",
    "                                                 # , flow_attribute='flow_total'\n",
    "                                                 # , flow_attribute = 'flow_total_lane_1')\n",
    "                                                 , flow_attribute='flow_total_lane'\n",
    "                                                 , flow_factor=1  # 0.1\n",
    "                                                 )\n",
    "    # Write counts in csv\n",
    "\n",
    "    filepath = tai.dirs['output_folder'] + 'network-data/links/' + str(data_reader.options['selected_date']) \\\n",
    "               + '-fresno-link-counts.csv'\n",
    "\n",
    "    counts_df = pd.DataFrame({'link_key': counts.keys(),\n",
    "                              'counts': counts.values(),\n",
    "                              'pems_ids': [link.pems_stations_ids for link in fresno_network.links]})\n",
    "    counts_df.to_csv(filepath, sep=',', encoding='utf-8', index=False, float_format='%.3f')\n",
    "\n",
    "    # Read counts from csv\n",
    "    counts_df = pd.read_csv(filepath, converters={\"link_key\": ast.literal_eval})\n",
    "\n",
    "    counts = dict(zip(counts_df['link_key'].values, counts_df['counts'].values))\n",
    "\n",
    "    # Load counts\n",
    "    fresno_network.load_traffic_counts(counts=counts)\n",
    "\n",
    "    # =============================================================================\n",
    "    # c) WRITE LINK FEATURES AND COUNTS\n",
    "    # =============================================================================\n",
    "    summary_table_links_df = tai.descriptive_statistics.summary_table_links(links=fresno_network.links)\n",
    "\n",
    "    summary_table_links_df.to_csv(tai.dirs['output_folder'] + 'network-data/links/'\n",
    "                     + str(data_reader.options['selected_date'])+ '-fresno-link-data.csv',\n",
    "                     sep=',', encoding='utf-8', index=False, float_format='%.3f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-transportAI",
   "language": "python",
   "name": "venv-transportai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
